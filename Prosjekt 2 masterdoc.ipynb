{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 : :)\n",
    "## Authors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warnings\n",
    "\n",
    "* Ikke endre uten at de andre vet det\n",
    "* **Alltid** gå på Kernel -> Restart & Clear Output\n",
    "* Ikke skriv utenfor cellene og sånn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import loader.py\n",
    "import plotting.py\n",
    "import spirals.py\n",
    "\n",
    "## GLOBAL CONSTS AND DEFINITIONS\n",
    "sigma = lambda x : np.tanh(x)         # Activation function\n",
    "eta = lambda x : 0.5*(1+np.tanh(x/2)) # Scalar function     \n",
    "sech = lambda x : 1/np.cosh(x)\n",
    "eta_ddx = lambda x :  0.25*sech(x/2)*sech(x/2) # Derivative of eta\n",
    "sigma_ddx = lambda x: sech(x)*sech(x) # Derivative of sigma\n",
    "h = 0.1                               # Step length          \n",
    "#_____________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 1/2*np.linalg.norm(Z-c)**2 # Cost function\n",
    "#But Z and C are not defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adam descent algorithn\n",
    "def AdamAlg():\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    alpha = 0.01\n",
    "    epsilon = 1e-8\n",
    "    v[0] = 0\n",
    "    m[0] = 0\n",
    "    k =3 #Debug\n",
    "    for j in range(1,k):\n",
    "        g[j] = uGrad(J(U[j]))\n",
    "        m[j] = beta1*m[j-1]+(1-beta1)*g[j]\n",
    "        v[j] = beta2*v[j-1]+(1-beta2)(g[j]*g[j])\n",
    "        mhat[j] = m[j]/(1-beta1**j)\n",
    "        vhat[j] = v[j]/(1-beta2**j)\n",
    "        U[j+1] -= alpha*mhat[j]/(np.sqrt(vhat[j])+e)\n",
    "def getGradients():\n",
    "    P[K] = omega*np.transpose((Z-c)*eta_ddx(np.transpose(Y[K])*omega+mu*One))\n",
    "    dJdMU = eta_ddx(np.transpose(np.transpose(Y[K])*omega+mu*One)*(Z-c))\n",
    "    dJdOmega = Y[K]*((Z-c)*eta_ddx(np.transpose(Y[K])*omega+mu))\n",
    "    for k in range(K,-1,2):\n",
    "        P[k-1] = P[k]+h*np.transpose(W[k-1])*(sigma_ddx(W[k-1]*Y[k-1]+b[k-1])*P[k])\n",
    "    for k in range(0,K-1):\n",
    "        dJdWk[k] = h(P[k+1]*sigma_ddx(W[k]*Y[k]+b[k]))*np.transpose(Y[k])\n",
    "        dJdBk[k] = h(P[k+1]*sigma_ddx(W[k]*Y[k]+b[k]))*One \n",
    "### THINGS IM UNCERTAIN ABOUT\n",
    "# Matrix mult @ or * for eq (9) and (10)??\n",
    "# First for-loop in getGradients, is the range correct?\n",
    "# Parameters for Adam alg?\n",
    "# What is Y[k]?\n",
    "# How to readData()?\n",
    "# How to check if converged? And when?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm\n",
    "K = 4 # Amount of layers, arbitrary number\n",
    "d = 3 # Ehh\n",
    "tau = 0.07 # learning parameter [0.01,0.1]\n",
    "Y0 = readData() # To be implemented\n",
    "Wk, bk = np.random.randn(k,d,d), np.random.randn(k,d,1)\n",
    "converged = False\n",
    "while not converged:\n",
    "    for k in range(1,K):\n",
    "        #Calculate Y[k] and save to memory\n",
    "    getGradients()\n",
    "    AdamAlg()\n",
    "    converged = True #Only one iteration when debugging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
